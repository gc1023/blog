WEBVTT

1
00:00:03.712 --> 00:00:08.185
You're now ready to see how to build one
layer of convolutional neural network.

2
00:00:08.185 --> 00:00:09.969
Let's go through an example.

3
00:00:12.344 --> 00:00:17.007
You've seen in a previous video
how to take a 3D volume and

4
00:00:17.007 --> 00:00:21.765
convolve it with say two
different filters in order to get,

5
00:00:21.765 --> 00:00:26.165
in this example,
two different four by four outputs.

6
00:00:30.733 --> 00:00:35.314
So let's say convolving
with the first filter

7
00:00:35.314 --> 00:00:39.080
gives this first four by four output.

8
00:00:40.750 --> 00:00:47.650
And convolving with this second filter
gives a different four by four output.

9
00:00:49.170 --> 00:00:56.540
The final thing to turn this into a
convolutional neural net layer is that for

10
00:00:56.540 --> 00:01:03.750
each of these we're going to add a bias,
so this is going to be a real number.

11
00:01:03.750 --> 00:01:08.200
And with Python broadcasting
you kind of add the same number

12
00:01:08.200 --> 00:01:11.750
to every one of these 16 elements.

13
00:01:11.750 --> 00:01:16.780
And then apply a non-linearity,
which for illustration,

14
00:01:16.780 --> 00:01:21.907
this is a [INAUDIBLE] non-linearity and
this gives you a four by

15
00:01:21.907 --> 00:01:27.055
four output after applying the bias and
the non-linearity.

16
00:01:27.055 --> 00:01:31.107
And then for this thing at the bottom
as well you add some different bias.

17
00:01:31.107 --> 00:01:33.240
Again, this is a real number.

18
00:01:33.240 --> 00:01:36.620
So you have the same real number,
it's all 16 numbers.

19
00:01:36.620 --> 00:01:40.920
And then apply some non-linearity,
let's say a real non-linearity.

20
00:01:40.920 --> 00:01:45.480
And this gives you a different
four by four output.

21
00:01:47.400 --> 00:01:49.120
Then, same as we did before.

22
00:01:49.120 --> 00:01:54.160
If you take this and
stack it up as follows,

23
00:01:55.250 --> 00:01:59.940
so you end up with a four
by four by two output.

24
00:01:59.940 --> 00:02:05.275
Then this computation where you've gone
from a six by six by three to a four

25
00:02:05.275 --> 00:02:10.370
by four by four, this is one layer
of a compositional neural network.

26
00:02:11.590 --> 00:02:16.390
So to map this back to one layer of
four propagation in the standard neural

27
00:02:16.390 --> 00:02:21.427
network, or a non-convolutional neural
network, remember that one step

28
00:02:21.427 --> 00:02:26.170
of forward prop was something like this,
right, z1 = w1 times a0.

29
00:02:26.170 --> 00:02:29.120
a0 was also equal to x.

30
00:02:29.120 --> 00:02:30.110
And then plus b1.

31
00:02:30.110 --> 00:02:34.700
And you apply the non-linearity to get a1.

32
00:02:34.700 --> 00:02:38.066
So that's g(z1).

33
00:02:38.066 --> 00:02:43.520
So this input here in this analogy,
this is a0, and this is x really.

34
00:02:44.770 --> 00:02:52.520
And these filters here displays
a variable similar to w1.

35
00:02:52.520 --> 00:02:57.460
And you remember during the convolution
operation, you're taking these 27 numbers,

36
00:02:57.460 --> 00:03:01.040
or really well 27 times 2,
because you have two filters.

37
00:03:01.040 --> 00:03:03.960
You're taking all of these numbers and
multiplying them, so

38
00:03:03.960 --> 00:03:09.605
you're really completing a linear
function to get this four by four matrix.

39
00:03:09.605 --> 00:03:16.202
So that four by four matrix,
the outputs of the convolution operation,

40
00:03:16.202 --> 00:03:20.107
that plays a role similar to w1 times a0.

41
00:03:20.107 --> 00:03:25.340
It's really maybe the operative of this
four by four as well as that four by four.

42
00:03:25.340 --> 00:03:28.370
And then the other thing
you do is add the bias.

43
00:03:29.690 --> 00:03:34.956
So this thing here, the four plane value,

44
00:03:34.956 --> 00:03:38.768
this plays a role similar to z.

45
00:03:38.768 --> 00:03:43.848
And then it's finally by applying the
non-linearity, it's kind of this, I guess.

46
00:03:43.848 --> 00:03:47.493
So this output, right, plays a role,

47
00:03:47.493 --> 00:03:53.390
this really becomes your
activation at the next layer.

48
00:03:53.390 --> 00:03:56.750
So this is how you go from a0 to a1.

49
00:03:56.750 --> 00:04:01.920
There's first the linear operation and
then convolution as all these multiply.

50
00:04:01.920 --> 00:04:06.410
So the convolution is really applying the
linear operation, and you add the biases,

51
00:04:06.410 --> 00:04:11.530
and you apply a value operation, and
you've gone from a six by six by

52
00:04:11.530 --> 00:04:17.030
three dimensional a0 through
one layer of a neural network

53
00:04:17.030 --> 00:04:21.520
to I guess a four by four
by two dimensional a1.

54
00:04:21.520 --> 00:04:27.760
So six by six by three has
gone to four by four by two.

55
00:04:27.760 --> 00:04:30.710
And so
that's one layer of a convolutional net.

56
00:04:33.571 --> 00:04:40.500
Now in this example we have two filters,
so we had two features as well.

57
00:04:41.930 --> 00:04:45.506
Which is why we want of our
output four by four by two.

58
00:04:45.506 --> 00:04:50.368
But if, for example, we instead had ten
filters instead of two then we would have

59
00:04:50.368 --> 00:04:54.430
one tap with four by four by
ten dimensional output volume.

60
00:04:54.430 --> 00:04:58.480
Because we'd be taking ten of these maps,
not just two of them, and

61
00:04:58.480 --> 00:05:02.620
stacking them up to form a four
by four by ten output volume.

62
00:05:02.620 --> 00:05:04.420
And that's what a1 would be.

63
00:05:05.640 --> 00:05:09.630
So to make sure you understand this,
let's go through an exercise.

64
00:05:09.630 --> 00:05:11.930
Let's suppose you have 10 filters,

65
00:05:11.930 --> 00:05:17.060
not just 2 filters, that are 3 x 3 x
3 in one layer of a neural network.

66
00:05:17.060 --> 00:05:19.045
How many parameters does this layer have?

67
00:05:21.000 --> 00:05:21.930
Well let's figure this out.

68
00:05:23.090 --> 00:05:28.060
Each filter is a three by
three by three volume.

69
00:05:28.060 --> 00:05:30.880
So three by three by three.

70
00:05:30.880 --> 00:05:35.429
So each filter has 27 parameters, right.

71
00:05:35.429 --> 00:05:36.930
So it's 27 numbers to be learned.

72
00:05:38.220 --> 00:05:44.050
And then plus the bias, so
that was the b parameters.

73
00:05:44.050 --> 00:05:50.218
So this gives you 28 parameters And

74
00:05:50.218 --> 00:05:54.032
then if you imagine that, on the previous
slide we had drawn two filters,

75
00:05:54.032 --> 00:05:57.060
but now if you imagine that you
actually have ten of these.

76
00:05:58.340 --> 00:06:03.632
1, 2, dot, dot, dot, 10 of these,
then all together you

77
00:06:03.632 --> 00:06:09.030
would have 28 times 10, so
that would be 280 parameters.

78
00:06:10.910 --> 00:06:16.241
Notice one nice thing about this is that
no matter how big the input image is,

79
00:06:16.241 --> 00:06:21.491
the image could be 1,000 by 1,000 or
5,000 by 5,000,

80
00:06:21.491 --> 00:06:26.350
but the number of parameters you
have still remains fixed as 280.

81
00:06:26.350 --> 00:06:30.836
And you can use these ten filters
to detect features, vertical edges,

82
00:06:30.836 --> 00:06:35.323
horizontal edges, maybe other features,
anywhere even in the very,

83
00:06:35.323 --> 00:06:39.240
very large image with just a very
small number of parameters.

84
00:06:40.920 --> 00:06:45.724
So this is really one property of
convolutional neural nets that makes less

85
00:06:45.724 --> 00:06:50.144
prone to over 15, that you could,
so long as you learn 10 feature

86
00:06:50.144 --> 00:06:54.661
detectors that work, you could apply
this even to very large images.

87
00:06:54.661 --> 00:06:57.669
And the number of parameters
still remains fixed and

88
00:06:57.669 --> 00:07:00.490
relatively small, as 280 in this example.

89
00:07:00.490 --> 00:07:02.784
All right, so to wrap up this video,

90
00:07:02.784 --> 00:07:07.668
let's just summarize the notation we're
going to use to describe one layer,

91
00:07:07.668 --> 00:07:12.050
to describe a convolutional layer,
in convolution neural network.

92
00:07:12.050 --> 00:07:14.465
So layer l is a convolutional layer.

93
00:07:14.465 --> 00:07:18.555
I'm going to use f superscript
[l] to denote the filter size.

94
00:07:18.555 --> 00:07:23.235
So previously we've been saying
the filters are f by f, and

95
00:07:23.235 --> 00:07:28.525
now this superscript [l] just denotes
that this is a filter size as an f

96
00:07:28.525 --> 00:07:31.060
by f filter in the layer l.

97
00:07:31.060 --> 00:07:39.691
And as usual the superscript [l] is the
notation we're using to refer to layer l.

98
00:07:39.691 --> 00:07:43.084
going to use p[l] to denote
the amount of padding, and again,

99
00:07:43.084 --> 00:07:46.478
the amount of padding can also
be specified just by saying that

100
00:07:46.478 --> 00:07:49.900
you want a valid convolution,
which means no padding.

101
00:07:49.900 --> 00:07:53.380
Or a same convolution,
which means you choose the padding so

102
00:07:53.380 --> 00:07:57.910
that the output size has the same
height and width as the input size.

103
00:07:59.000 --> 00:08:01.590
And then I'm going to use
s[l] to denote the stride.

104
00:08:03.250 --> 00:08:09.360
Now the input to this layer is
going to be some dimension.

105
00:08:09.360 --> 00:08:14.104
There's going to be some n by n

106
00:08:14.104 --> 00:08:18.590
by number of channels
in the previous layer.

107
00:08:18.590 --> 00:08:22.335
Now I'm going to modify this notation
a little bit, I'm going to use superscript

108
00:08:22.335 --> 00:08:27.898
l-1 because that's the activation
from the previous layer up.

109
00:08:27.898 --> 00:08:35.490
[l-1] times nc[l-1].

110
00:08:35.490 --> 00:08:37.970
And in the examples so far,

111
00:08:37.970 --> 00:08:40.960
we've been just using images
with the same height and width.

112
00:08:40.960 --> 00:08:45.430
But in case the height and width might
differ, I'm going to use superscript h and

113
00:08:45.430 --> 00:08:50.370
superscript w to denote the height and
width of the input of the previous layer.

114
00:08:51.560 --> 00:08:56.340
All right, so
in layer l the size of the volume

115
00:08:56.340 --> 00:09:01.110
will be nh by nw by nc,
with superscript [l].

116
00:09:01.110 --> 00:09:06.348
It's just in layer l the input to
this layer is whatever you had

117
00:09:06.348 --> 00:09:12.108
from the previous layer, so
that's why you have o minus one there.

118
00:09:12.108 --> 00:09:13.900
And then the neural network,
excuse me, this layer and

119
00:09:13.900 --> 00:09:16.586
then this layer of the neural network will
output, will itself output the volume.

120
00:09:16.586 --> 00:09:23.046
So that would be nh of l
by nw of l by nc of l,

121
00:09:23.046 --> 00:09:27.720
l be the size of the output.

122
00:09:27.720 --> 00:09:33.210
And so whereas we have previously
said that the output volume size,

123
00:09:34.440 --> 00:09:38.537
or at least the height and
width, is given by this formula,

124
00:09:38.537 --> 00:09:42.717
(n+2p-f over

125
00:09:42.717 --> 00:09:47.950
s)+1, and then take the full of that or
round it down.

126
00:09:47.950 --> 00:09:53.040
And in this new notation what
we have is that the output's

127
00:09:54.680 --> 00:10:00.030
volume that is in layer l is
going to be the dimension

128
00:10:00.030 --> 00:10:06.520
from the previous layer plus
the padding we're using in this layer l

129
00:10:06.520 --> 00:10:12.260
minus the filter size we're
using in this layer l and so on.

130
00:10:12.260 --> 00:10:16.580
And technically this is true for
the height, right,

131
00:10:16.580 --> 00:10:20.690
so the height of the output
volume is given by this.

132
00:10:20.690 --> 00:10:23.010
And you can compute it with
this formula on the right.

133
00:10:23.010 --> 00:10:26.700
And the same is true for
the width as well, so you cross out h and

134
00:10:26.700 --> 00:10:28.270
throw in w as well.

135
00:10:28.270 --> 00:10:31.970
Then the same formula with either
the height or the width plugged in.

136
00:10:31.970 --> 00:10:34.800
We're also computing the height or
the width of the output volume.

137
00:10:36.570 --> 00:10:43.020
So that's how nhl-1 relates to nh1,
and wl-1 relates to nwl.

138
00:10:44.120 --> 00:10:45.940
Now how about the number of channels?

139
00:10:45.940 --> 00:10:47.320
Where do those numbers come from?

140
00:10:47.320 --> 00:10:48.490
Let's take a look.

141
00:10:50.030 --> 00:10:56.230
If the upward volume has this depth,
well we know from the previous

142
00:10:56.230 --> 00:11:01.880
examples that that's equal to the number
of filters we have in that layer.

143
00:11:01.880 --> 00:11:03.032
So we had two filters,

144
00:11:03.032 --> 00:11:06.870
the output volume was four by four
by four by two, was two dimensional.

145
00:11:06.870 --> 00:11:10.536
And if you have ten filters, then
the output volume was four by four by ten.

146
00:11:10.536 --> 00:11:17.370
So this, the number of
channels in the output volume,

147
00:11:17.370 --> 00:11:21.720
that's just the number of filters we're
using in this layer of the neural network.

148
00:11:23.480 --> 00:11:26.920
Next, how about the size of each filter?

149
00:11:26.920 --> 00:11:32.849
Well each filter is going to be fl,
by fl by one other number, right.

150
00:11:32.849 --> 00:11:34.760
So what is this lost number?

151
00:11:34.760 --> 00:11:37.713
Well we saw that you
need to convolve a six by

152
00:11:37.713 --> 00:11:41.580
six by three image with a three
by three by three filter.

153
00:11:43.070 --> 00:11:46.170
And so
the number of channels in your filter

154
00:11:46.170 --> 00:11:49.470
must match the number of
channels in your input.

155
00:11:49.470 --> 00:11:54.992
So this number should match that number,
right,

156
00:11:54.992 --> 00:12:01.570
which is why each filter is
going to be fl by fl by nc, l-1.

157
00:12:03.140 --> 00:12:06.600
And the output of this layer
after applying the biases and

158
00:12:06.600 --> 00:12:11.745
non-linearity is going to be
the activations of this layer, al.

159
00:12:11.745 --> 00:12:15.023
And that we've already seen
will be this dimension, right.

160
00:12:15.023 --> 00:12:20.042
The al will be a 3D volume

161
00:12:20.042 --> 00:12:25.302
that's n hl by nwl by ncl.

162
00:12:25.302 --> 00:12:30.018
And when you are using
a [INAUDIBLE] instrumentation or

163
00:12:30.018 --> 00:12:35.048
you know [INAUDIBLE] or
mini [INAUDIBLE], then you actually

164
00:12:35.048 --> 00:12:40.630
output al which is a set of m
activations if you have m examples.

165
00:12:40.630 --> 00:12:47.998
So that will be m by nhl by nwl by ncl,
right.

166
00:12:47.998 --> 00:12:50.110
If say you're using.

167
00:12:50.110 --> 00:12:54.780
And in the parameter sizes, this would
be ordering of the variables and

168
00:12:54.780 --> 00:12:58.870
we have the index and
the training examples first and

169
00:12:58.870 --> 00:13:02.420
then these three variables.

170
00:13:02.420 --> 00:13:07.590
Next, how about the weights or
the parameters or kind of the w parameter?

171
00:13:07.590 --> 00:13:10.270
Well we saw already what
the filter dimension is.

172
00:13:10.270 --> 00:13:15.145
So the filters are going to
be fl by fl by nc l-1,

173
00:13:15.145 --> 00:13:20.290
but that's the dimension of one filter.

174
00:13:20.290 --> 00:13:22.220
How many filters do we have?

175
00:13:22.220 --> 00:13:24.450
Well this is the total number of filters,
so

176
00:13:24.450 --> 00:13:27.810
the weights,
really all of the filters put together,

177
00:13:27.810 --> 00:13:33.470
will have dimension given by this times
the total number of filters, right.

178
00:13:33.470 --> 00:13:38.322
Because this loss
quantity is the number of

179
00:13:38.322 --> 00:13:43.750
filters, In layer l.

180
00:13:45.680 --> 00:13:48.710
And then finally you have
the bias parameters, and

181
00:13:48.710 --> 00:13:54.100
you have one bias parameter,
one real number, for each filter.

182
00:13:54.100 --> 00:13:57.970
So you're going to have,
the bias will have this many variables,

183
00:13:57.970 --> 00:14:00.810
it's just a vector of this dimension.

184
00:14:00.810 --> 00:14:05.762
Although later on we'll see that
in the code it will be more

185
00:14:05.762 --> 00:14:11.120
convenient represented as a one
by one by one by ncl dimension,

186
00:14:11.120 --> 00:14:15.891
four dimensional matrix or
four dimensional.

187
00:14:15.891 --> 00:14:18.877
So I know that was a lot of notation,

188
00:14:18.877 --> 00:14:23.320
and, This is the convention I've used for
the most part.

189
00:14:23.320 --> 00:14:27.600
I just want to mention, in case you search
online and look at open source codes,

190
00:14:27.600 --> 00:14:32.880
there isn't a completely universal
standard convention about the ordering

191
00:14:32.880 --> 00:14:34.180
of height, width, and channel.

192
00:14:34.180 --> 00:14:36.737
So if you look on source code on GitHub or

193
00:14:36.737 --> 00:14:40.999
read some of the open source
implementations, you find that some

194
00:14:40.999 --> 00:14:45.966
authors use this authoring standard
where you first put the channel first.

195
00:14:45.966 --> 00:14:48.190
And you sometimes see that all
through the other variables.

196
00:14:48.190 --> 00:14:51.260
And in fact in some current frameworks,
actually in multiple current frameworks,

197
00:14:51.260 --> 00:14:55.580
there's actually a variable or a parameter
whether you want to list the number of

198
00:14:55.580 --> 00:15:00.290
channels first list the number of channels
lost when indexing into these volumes.

199
00:15:02.060 --> 00:15:08.070
And I think both of these conventions
work okay so long as you are consistent.

200
00:15:09.490 --> 00:15:14.390
And unfortunately maybe this
is one piece of notation

201
00:15:14.390 --> 00:15:18.600
where there isn't consensus in
the deep learning literature.

202
00:15:18.600 --> 00:15:22.212
But I'm going to use this convention for
these videos.

203
00:15:24.509 --> 00:15:29.110
Where we list height and then width and
then the number of channels lost.

204
00:15:30.870 --> 00:15:34.800
So I know there was suddenly a lot of new
notations introduced, but you thinking,

205
00:15:34.800 --> 00:15:36.110
wow, there is a lot of notation.

206
00:15:36.110 --> 00:15:37.870
How do I need to remember all of these?

207
00:15:37.870 --> 00:15:41.670
Don't worry about it, you don't need
to remember all of this notation, and

208
00:15:41.670 --> 00:15:46.255
through this week's exercises you become
more familiar with it at that time.

209
00:15:46.255 --> 00:15:50.890
But the key point I hope you take away
from this video is just how one layer

210
00:15:50.890 --> 00:15:54.940
of the compositional neural network works,
and the computations involved in taking

211
00:15:54.940 --> 00:16:00.120
the activations of one layer and mapping
that to the activations of the next layer.

212
00:16:00.120 --> 00:16:04.020
And next, now that you know how one layer
of the compositional neural network works,

213
00:16:04.020 --> 00:16:07.470
let's stack a bunch of these
together to actually form a deeper

214
00:16:07.470 --> 00:16:09.040
compositional neural network.

215
00:16:09.040 --> 00:16:10.110
Let's go on to the next video to