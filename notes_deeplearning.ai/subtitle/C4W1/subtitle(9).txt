You now know pretty much all the building
blocks to building a full convolutional neural network. Let's look at an example. Let's say that you're inputting an image,
which is 32 by 32 by 3. So it's an RGB image, and maybe you're trying to do
handwritten digit recognition. So you have a number like 7,
and 32 by 32 RGB image, and you're trying to recognize which one
of the 10 digits from 0 to 9 this is. Let's build a neural network to do this. And what I'm going to use
in this slide is inspired. It's actually quite similar to one of
the classic networks called LeNet-5, which was created by
Yann LeCun many years ago. What I've shown here isn't exactly
LeNet-5, but is inspired by it. But many of the parameter
choices was inspired by it. So we have a 32 by 32 by 3 input. Let's say that the first layer uses a 5 by
5 filter and a stride of 1 and no padding. So the output of this layer
if you use 6 filters, would be 28 by 28 by 6. And we're going to call this layer CONV 1. See it applied six filters,
added buyers apply the non-linearity, maybe a real non-linearity,
and that's the CONV 1 output. Next, let's apply a pooling layer. So I'm going to apply max pooling here, and let's use f = 2, s = 2. Well, now don't write a padding
as padding is equal to zero. Next, let's apply a pooling layer. I'm going to apply, let's say max pooling
with a 2 by 2 filter, and the stride = 2. So this should reduce the height and width
of the representation by a factor of 2. So 28 by 28 now becomes 14 by 14. And the number of channels remains
the same so 14 by 14 by 6. And we're going to call this,
the POOL 1 output. So, it turns out that in
the literature of a Conv net, there are two conventions which are slightly in
consistence about what you call a layer. One convention is that
this is called one layer, so this will be Layer 1
of the neural network. Another convention would be to count
the Conv layer as a layer, and the Pool layer as a layer. When people report a number of layers in a
neural network, usually people report just the number of layers that have weights,
that have parameters. And because the pooling layer has
no weights, has no parameters, only a few hyper parameters. I'm going to use the convention
that the CONV 1 and POOL 1 here together,
I'm going to treat that as Layer 1. Although sometimes you see people,
if you read articles online, or read research papers,
you hear about the Conv layer and the pooling layer as if they
are two separate layers. But this is maybe two slightly
inconsistent notation terminologies. But when I count layers, I'm just
going to count layers that have ways, so they cheap over these
together as Layer 1. And the name CONV 1 and
POOL 1 that we're going to use here, the one at the end also refers to the fact
that I view both of these as part of Layer 1 of the neural network. And POOL 1 is grouped into Layer 1,
because it doesn't have its own weights. Next, given a 14 by 14 by 6 volume, let's apply another
convolutional layer to it. Let's use a filter size that's 5 by 5,
and let's use a stride of 1, and let's use 10 filters this time. So now you end up with, A 10 by 10 by 10 volume. We'll call this CONV 2. And then in this network, let's do max pooling with f = 2,
s = 2 again. So you could probably
guess the output of this. f = 2, s = 2,
this should reduce the height and width by a factor of 2, so
you're left with 5 by 5 by 10. And so I'm going to call this Pool 2,
and in our convention, this is Layer 2 of the neural network. Now, let's apply another
convolutional layer to this. I'm going to use a 5.5 filter,
so f = 5, and a stride is 1, and
we're going to down write the padding. It means there's no padding. And this will give you the CONV 2 output,
unless you're 16 filters. So this will be a 10 by 10
by 16 dimensional output, so we will look at that. And this is the CONV 2 layer,
and then let's apply our mass pooling to this with f = 2,
s = 2. You can probably guess the output of this,
right? The 10 by 10 by 16 with mass
pooling with f = 2, s = 2. This will half the height and
width, right? You can probably guess the result of this,
right? Max pooling with f = 2, s = 2,
this should half the height and width so
you end up with a 5 by 5 by 16 volume. Same number of channels as before,
we're going to call this POOL 2. And in our convention, this is Layer 2, because this has one set of weights and
a CONV 2 layer. Now, 5 by 5 by 16,
5 times 5 times 16 is equal to 400. So let's now fatten our pool 2 into
a 400 by 1 dimensional vector. And so we'll think of this fattening
result into just a set of neurons like so. And what we're going to do is
then take this 400 units, and let's build the next layer,
As having 120 units. So this is actually our
first fully connected layer, I'm going to call this FC3, because we have 400 units densely
connected to 120 units. So this fully connected unit,
this fully connected layer is just like the single neural network layer that
you saw in courses one and two. This is just a standard neural
network where you have a weight matrix that's called W[3] of
dimension 120 by 400, right? And this is called fully connected,
because each of the 400 units here is connected to each of the 120 units here,
and you'd also have a bias parameter. Yes, that's going to be just 120
dimensional, because you have 120 outputs. And then lastly, let's take the 120
units and add another layer. This time, a little bit smaller. But let's say we have 84 units here. We're going to call this
fully connected layer 4. And finally, you now have 84 row numbers
that you can feed to a softmax unit. And if you're trying to do
handwritten digit recognition, to recognize is it handwritten 0,
1, 2, and so on up to 9, then this would
be a softmax with 10 outputs. So, this is a reasonably typical example of what a convolutional neural
network might look like. And I know this seems like there
are a lot of hyperparameters. We'll give you some more
specific suggestions later for how to choose these types
of hyperparameters. Maybe one common guideline is to
actually not try to invent your own settings of hyperparameters, but to look in the literature to see what
hyperparameters you work for others. And to just choose an architecture that
has worked well for someone else,, and there's a chance that will work for
your application as well. We'll say more about that next week. But for now, I just want to point out
that, as you go deeper in the neural network, usually nH and nW,
the height and width, will decrease. Pointed this out earlier, that it goes from 32 by 32 to 28 by
28 to 14 by 14 to 10 by 10 to 5 by 5. So as you go deeper, usually the height
and width will decrease, whereas, the number of channels will increase, going from 3 to 6 to 16, and then you're
fully connected layer is at the end. And another pretty common pattern you
see in neural networks is they have CONV layers, maybe one or more CONV
layers followed by a pooling layer. And then one or more CONV layers
followed by a pooling layer. And then at the end you have
a few fully connected layers, and then followed by maybe a softmax. And this is another pretty common
pattern you see in neural networks. So let's just go through for this neural network some more details
of what are the activation shape, the activation size, and
the number of parameters in this network. So the input was 32 by 30 by 3, and
you multiply out those numbers. You should get 3,072. So the activation, a 0,
has dimension 3,072. Well, it's really 32 by 32 by 3. And there are no parameters,
I guess in the input layer. And as you look at the different layers, feel free to work out
the details yourself. These are the activation shape and the activation sizes of
these different layers. So just to point out a few things,
first, notice that the pooling layers, the max pooling layers
don't have any parameters. Second, notice that the CONV layers
tend to have relatively few parameters, as we discussed in earlier video. And, in fact, a lot of the parameters
tend to be in the fully connected layers of the neural network. And then you notice also that
the activation size tends to maybe go down gradually as you go
deeper in the neural network. If it drops too quickly, that's usually
not great for performance as well. So it starts and
the first day I was 6,000 and 1,600 and then slowly falls into 84. Until finally,
you have your softmax output. You find that a lot of
will have properties, will have patterns similar to these. So you've now seen the basic
building blocks of neural networks, a convolution neural networks. The CONV layer, the POOLING layer,
and the Fully Connected layer. A lot of computer vision research
has gone into figuring out how to put together these basic building
blocks to build effective neural networks. And putting these things together
actually requires quite a bit of insight. I think that one of the best ways for you to gain intuition about how
to put these things together is to see a number of concrete
examples of how others have done it. So what I want to do next week is show you
a few concrete examples even beyond this first one that you just saw on people have
successfully put these things together to develop very effective neural networks. And through those videos next week, I hope
that'll help you hone your own intuitions about how these things are built, as well
as give you concrete concrete examples of architectures that maybe you can just use,
exactly, as developed by someone else for
your own application. So we'll do that next week. But before wrapping up this weeks videos,
just one last thing which is I want to talk a little bit in the next video about
why you might want to use convolutions. So the benefits and advantages of using
convolutions, as well as how to put it all together, how to check neural network,
like the one you just saw, and actually train it on the training set to perform
image recognition or some other task. So that,
let's go on to the last video of this week