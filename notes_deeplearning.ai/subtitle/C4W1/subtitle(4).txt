Strided convolutions is another piece of the basic building block of convolutions as used in convolutional neural networks. Let me show you an example. Let's say you want to convolve this 7 x 7 image with this 3 x 3 filter, except that instead of doing it the usual way, we're going to do it with a stride of two. What that means is you take the element-wise product as usual in this upper left 3 x 3 region and then multiply and add and that gives you 91. But then instead of stepping the blue box over by one step, we're going to step it over by two steps. So we're going to make it hop over two steps like so. Notice how the upper left hand corner has gone from the start to this start, jumping over one position. And then you do the usual element-wise product then summing, and that gives you, it turns out,100. And now, we're going to do that again and make the blue box jump over by two steps. So you end up there and that gives you 83. And now, when you go to the next row, you again actually take two steps instead of one step. So we're going to move the blue box over there, notice how we're skipping over one of the positions and then this gives you 69. And now you again step over two steps. This gives you 91, and so on. So 127 and then for the final row, 44, 72, and 74. So in this example, we convolve with a 7 x 7 matrix with a 3 x 3 matrix and we get a 3 x 3 output. So the input and output dimensions turns out to be governed by the following formula. If you have n x n image then you convolve with an f x f filter, and if you use padding p, and stride s. So in this example, s = 2, then you end up with an output that is n plus 2 to the p minus f. And now because you're stepping s steps at a time instead of just one step at a time, you now divide by s plus 1 and then by the same thing. So in our example, we have 7 plus zero minus 3 divided 2, s stripe plus 1 equals, let's see, that's 4 over 2 plus 1 = 3, which is why we wound up with this 3 x 3 output. Now just one last detail which is, what if this fraction is not an integer? In that case, we're going to round this down. So this notation denotes the floor of something. So this is also called the floor of z. It means taking z and rounding down to the nearest integer. And if the way this is implemented is that you take this type of blue box multiplication only if the blue box is fully contained within the image or the image plus the padding. And if any of this blue box, kind of, part of it hangs outside, then you just do not do that computation. Then it turns out that that's the convention that your 3 x 3 filter must lie entirely within your image or the image plus padding region before there's a corresponding output generated, that's the convention. Then the right thing to do to compute the output dimension is to round down in case this n + 2 p - f over s is not an integer. So just to summarize the dimensions, if you have an n x n matrix or n x n image that you convolve with an f x f matrix, an f x f filter with padding p, and stride s, then the output size will have this dimension. And it is nice we can choose all of these numbers so that that is an integer, although sometimes, you don't have to do that and rounding down is just fine as well. But please feel free to work through a few examples of values of n, f, p, and s for yourself to convince yourself if you want that this formula is correct for the output size. Now before moving on, there is a technical comment I want to make about cross-correlations versus convolutions. And this won't affect what you have to do to implement convolutional neural networks, but depending on if you read a different math textbook or signal processing textbook, there is one other possible inconsistency in your notation. Which is that, if you look at a typical math textbook, the way that a convolution is defined, before doing the element-wise product and summing, there's actually one other step that you will first take, which is to convolve this 6 x 6 matrix with the 3 x 3 filter. You will first take the 3 x 3 filter and flip it on the horizontal as well as the vertical axis. So this 3, 4, 5, 1, 0, 2, minus 1, 9, 7, will become, 3 goes here, 4 goes there, 5 goes there. And then the second row becomes this 1, 0, 2 minus 1, 9, 7. This is really taking the 3 x 3 filter and mirroring it, both on the vertical and horizontal axes. And then there was this flipped matrix that you would then copy over here. So to compute the output, you would take 2 times 7 plus 3 times 2, plus 7 times 5, and so on. You actually multiply out the elements of this flipped matrix in order to compute the upper lefthand-most elements of the 4 x 4 output as follows. And then you take those nine numbers and shift them over by one, shift them over by one, and so on. So the way we've defined the convolution operation in these videos is that we've skipped this mirroring operation. And technically, what we're actually doing, really, the operation we've been using for the last few videos is sometimes called cross-correlation instead of convolution. That in deep learning literature, by convention, we just call this a convolution operation. So just to summarize, by convention in machine learning, we usually do not bother with this flipping operation and technically, this operation is maybe better called cross-correlation, but most of the deep learning literature just calls this the convolution operator. And so, I'm going to use that convention in these videos as well. And if you read a lot of the machine learning literature, you find most people just call this the convolution operator without bothering to use these flips. And it turns out that in signal processing or in certain branches of mathematics, doing the flipping in the definition of convolution causes convolution operator to enjoy this property, that A convolved with B convolved with C is equal to A convolved with B, convolved with C, and this is called associativity in mathematics. And this is nice for some signal processing applications, but for deep neural networks, it really doesn't matter. And so, omitting this double mirroring operation just simplifies the code and makes the neural networks work just as well. And by convention, most of us just call this convolution, even though the mathematicians prefer we call this cross-correlation sometimes. But this should not affect anything you have to implement in the following exercises and should not affect your ability to read and understand the deep learning literature. So you've now seen how to carry out convolutions and you've seen how to use padding as well as strides for convolutions. But so far, all we've been using is convolutions over matrices, like over a 6 x 6 matrix. In the next video, you see how to carry out convolutions over volume. And this will make what you can do with convolutions suddenly much more powerful. Let's go on to the next-