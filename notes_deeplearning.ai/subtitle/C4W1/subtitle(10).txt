For this final for this week, let's talk
a little bit about why convolutions are so useful when you include them
in your neural networks. And then finally, let's briefly talk about
how to put this all together and how you could train a convolutional neural network
when you have a labeled training set. I think the two main advantages
of convolutional layers over just using fully connected layers. And the advantages are parameter
sharing and sparsity of connections. Let me illustrate through example. Let's say you have a 32 by
32 by 3 dimensional image. And this actually comes from
the example from the previous video, but let's say you use a 5
by 5 filter with 6 filters. And so this gives you a 28 by
28 by 6 dimensional outputs. So 32 by 32 by 3 is 3,072. And 28 by 28 by 6 if you multiply
out those numbers is 4,704. And so, if you were to create
a neural network with 3,072 units in one layer, and
4,704 units in the next layer, and you were to connect every one of
these neurons, then the weight matrix, the number parentheses in
the weight matrix would be 3,072 times 4,704, which is about 14 million. So that's just a lot of
parameters to train. And today you can train your networks with
even more parameters than 14 million, but considering that this is
just a pretty small image, this is a lot of parameters to train. And of course if this were to
be a 1,000 by 1,000 image, then this weight matrix will
just become invisibly large. But if you look at the number
parameters in this convolutional layer. Each filter is 5 by 5. So each filter has 25 parameters
plus the base parameter means you have 26 parameters per filter,
and you have 6 filters. So the total number of parameters is
that which is equal to 156 parameters. And so the number of parameters in
this ConvLayer remains quite small. And the reason that a ConvNets has routed
this small parameter is two reasons, one is parameter sharing. And parameter sharing is
motivated by the observation that feature detector such as vertical edge
detector that's useful in one part of the image is probably useful
in another part of the image. And what that means is that if you've
figured out say a 3 by 3 filter for detecting vertical edges, you can then
apply the same 3 by 3 filter over here, and then the next position over, and
in the next position over and so on. And so each of these feature detectors,
each of these outputs can use the same parameters in lots of
different positions in your input image in order to detect say a vertical edge or
some other feature. And I think this is true for low level
features like edges as well as for higher level features like maybe detecting
the eye that indicates a face or a cat or something is there. But being able to share in this case the
same nine parameters to compute all 16 of these outputs as one of the ways
the number of parameters is reduced. And it also just seems intuitive
that a future detector like a vertical edge detector computed for
the upper left hand corner of the image. The same feature seems like
it will probably be useful, has a good chance of being useful for
the lower right hand corner of the image. So maybe you don't need to learn
separate feature detectors for the upper left hand and
lower right hand corners of the image. And maybe you do have a data set
where the upper left hand corner and the lower right hand corner
have different distributions or maybe look a little bit different but
they might be similar enough. They're sharing feature detectors in
all across the image works just fine. The second way that ConvNets get away
with having relatively few parameters is by having sparse connections. So here's what I mean. If you look at this 0,
this is computed via 3 by 3 convolution. And so it depends only on this
3 by 3 input degrader cells. So is as if this open unit
on the right is connected only to 9 out of these 6 by 6,
36 input features. And in particular, the rest of these
pixel values, all of these pixel values, all of these pixel values do not
have any effect on that output. So that's what I mean by
small sea of connections. As another example, this output depends only on these nine input features. And so it is as if only those nine input
features are connected to this output, and the other pixels just don't
affect this output at all. And so, through these two mechanisms, a
neural network has a lot fewer parameters, which allows it to be trained
with smaller training sets, and it's less prone to be over-fitting. And sometimes you also hear about
convolutional neural networks being very good at capturing
translation of the areas. And that's the observation
that a picture of a cat shifted a couple of pixels to the right
is still pretty clearly a cat. And a convolutional structure helps
the neural network encode the fact that an image shifted a few pixels should
result in pretty similar features, and should probably be assigned
the same output label. And the fact that you're
applying the same filter yields all the position of the image. Of both the early layers and
in the later layers, that hopes in your network automatically
learn to be with a more robust. So to better capture this desirable
property of translation and variance. So these are maybe are couple
of reasons why convolutions or convolutional neural networks works so
well in computer vision. Finally, let's put it all together and see
how you can train one of these networks. Let's say you want to
build a cat detector, and you have a label training set as
follows where now x is a image. And the ys can be binary labels or
one of k causes. And let's say you've chosen
a convolutional neural network structure. Maybe started the image and having
convolution and pulling layers, and then some fully connected layers, followed by
a soft max output that then outputs y hat. The ConvLayers and
the fully connected layers will have various parameters to help
you as well as biases b. And so, any set in the parameters
therefore lets you define a cost function similar to what we have
seen in the previous courses, where were randomly
initialized parameters w and b. You can compute the cost J as the sum
of losses of the neural network's predictions on your entire training set,
maybe divide it by m. So to train this neural network all
you need to do is then use gradient descents or some other algorithm
like gradient descent with momentum, or armlets prop or
add them or something else. In order to optimize all
the parameters in the neural network to try to reduce the cost function J. And you find that if you do this, you can
build a very effective cat detector or some other detector. So congratulations on
finishing this week's videos. You've now seen all the basic building
blocks of a convolution neural network, and how to put them together into
an effective image recognition system. In this weeks' programming exercises, I
think all of these things will become more concrete and you get a chance to practice
implementing these things yourself and seeing it work for yourself. Next week, we'll continue to go deeper
into convolutional neural networks. I mentioned earlier that there's just
a lot of hyper parameters in convolutional neural networks. So what I want to do next week is show
you a few concrete examples of some of the most effective convolutional
neural networks, so you can start to recognize the patterns of what types
of network architectures are effective. And one thing that people
will often do is just take the architecture that
someone else has found and published in a research paper and
just use that for your application. And so by seeing some more
concrete examples next week, you also learn how to do that better. And beyond that, next week, we'll also just get that intuitions
about what makes ConvNets work well. And then in the rest of the course,
we'll also see a variety of other computer vision applications such as object
detection and neural static transfer. How to create new forms of artwork
using these types of algorithms. So that's it for this week. Best of luck with the homeworks, and
I look forward to seeing you next week.