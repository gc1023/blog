WEBVTT

1
00:00:00.620 --> 00:00:05.480
Other than convolutional layers,
ConvNets often also use pooling layers to

2
00:00:05.480 --> 00:00:08.700
reduce the size of their representation
to speed up computation,

3
00:00:08.700 --> 00:00:12.180
as well as to make some of the features
it detects a bit more robust.

4
00:00:12.180 --> 00:00:12.730
Let's take a look.

5
00:00:14.050 --> 00:00:16.710
Let's go through an example of pooling,
and

6
00:00:16.710 --> 00:00:19.880
then we'll talk about why
you might want to do this.

7
00:00:20.920 --> 00:00:25.240
Suppose you have a 4x4 input, and

8
00:00:25.240 --> 00:00:28.160
you want to apply a type of
pooling called max pooling.

9
00:00:29.450 --> 00:00:33.040
And the output of this particular
implementation of max pooling

10
00:00:33.040 --> 00:00:35.330
will be a 2x2 output.

11
00:00:35.330 --> 00:00:37.360
And the way you do that is quite simple.

12
00:00:37.360 --> 00:00:40.380
Take your 4x4 input and
break it into different regions.

13
00:00:40.380 --> 00:00:43.672
And I'm going to color
the four regions as follows.

14
00:00:43.672 --> 00:00:46.484
And then in the output, which is 2x2,

15
00:00:46.484 --> 00:00:52.137
each of the outputs will just be the max
from the correspondingly shaded region.

16
00:00:52.137 --> 00:00:58.185
So in the upper left, I I guess
a max of these four numbers is 9.

17
00:00:58.185 --> 00:01:01.995
Upper right, and
the max of blue numbers is 2.

18
00:01:01.995 --> 00:01:07.170
Lower left, the biggest number is 6, and
lower right, the biggest number is 3.

19
00:01:07.170 --> 00:01:10.519
So to compute each
the numbers on the right,

20
00:01:10.519 --> 00:01:13.425
we took the max over by a 2x2 region.

21
00:01:13.425 --> 00:01:19.809
So this is as if you're applying
a filter size of 2 because you're

22
00:01:19.809 --> 00:01:25.406
taking 2x2 regions, and
you're taking a stride of 2.

23
00:01:25.406 --> 00:01:31.542
So these are actually the Hyperparameters,
Of max pooling.

24
00:01:31.542 --> 00:01:39.680
Because we start from this filter size is
like a 2x2 region, that gives you the 9.

25
00:01:39.680 --> 00:01:45.590
And then you step it over two steps to
look at this region to give you the 2.

26
00:01:45.590 --> 00:01:49.025
And then for the next row,
you step down two steps to give you the 6.

27
00:01:49.025 --> 00:01:52.540
And then you step to the right
by two steps to give you the 3.

28
00:01:52.540 --> 00:01:55.780
So because the squares are 2x2,
f is equal to 2.

29
00:01:55.780 --> 00:01:58.060
And because you stride by 2,
s is equal to 2.

30
00:01:58.060 --> 00:02:05.248
So here's the intuition behind
what max pooling is doing.

31
00:02:05.248 --> 00:02:09.540
If you think of this 4x4 input
as some set of features.

32
00:02:09.540 --> 00:02:14.494
Maybe not, if you think of
this 4x4 region as some set

33
00:02:14.494 --> 00:02:19.290
of features, deactivations in
some layer of the neural network,

34
00:02:19.290 --> 00:02:23.690
then a large number means that it's
maybe detected a particular feature.

35
00:02:23.690 --> 00:02:28.166
Right, so the upper left hand quadrant has
this particular feature, maybe a vertical

36
00:02:28.166 --> 00:02:31.921
edge, or maybe an eye, or a we're
scared of it trying to detect the cap.

37
00:02:31.921 --> 00:02:35.925
But clearly that feature exists
in the upper left hand quadrant.

38
00:02:35.925 --> 00:02:40.055
Whereas this feature,
maybe there's a cat eye detector,

39
00:02:40.055 --> 00:02:44.070
whereas this feature doesn't really
exist in the upper right hand quadrant.

40
00:02:44.070 --> 00:02:48.140
So what the max operation does is so long
as the feature is detected anywhere in one

41
00:02:48.140 --> 00:02:53.680
of these quadrants, it then remains
preserved in the output of Max pooling.

42
00:02:53.680 --> 00:02:56.910
So what the max operator
does is really it says,

43
00:02:56.910 --> 00:03:01.980
if the speech is detected anywhere in
this filter, then keep a high number.

44
00:03:01.980 --> 00:03:04.067
But if this feature is not detected, so

45
00:03:04.067 --> 00:03:07.673
maybe the feature doesn't exist
in the upper right hand quadrant,

46
00:03:07.673 --> 00:03:11.150
then the max of all those numbers
is still itself quite small.

47
00:03:11.150 --> 00:03:14.070
So maybe that's the intuition
behind max pooling.

48
00:03:15.760 --> 00:03:20.304
But I have to admit, I think the main
reason people use max pooling is because

49
00:03:20.304 --> 00:03:23.670
it's been found in a lot of
experiments to work well.

50
00:03:23.670 --> 00:03:27.490
And the intuition I just described,
despite it being often cited,

51
00:03:27.490 --> 00:03:33.320
I don't know if anyone fully knows if
that's the real underlined reason.

52
00:03:33.320 --> 00:03:36.805
I don't know if anyone knows
that that's the real underlying

53
00:03:36.805 --> 00:03:39.695
reason that max pooling
works well in confidence.

54
00:03:39.695 --> 00:03:44.523
One interesting property of max pooling
is that it has a set of hyperparameters,

55
00:03:44.523 --> 00:03:47.080
but it has no parameters to learn, right.

56
00:03:47.080 --> 00:03:50.360
There's actually nothing for
gradient descent to learn.

57
00:03:50.360 --> 00:03:54.220
Once you've fix f and s,
it's just a fixed computation and

58
00:03:54.220 --> 00:03:56.120
gradient descent doesn't change anything.

59
00:03:57.370 --> 00:04:00.309
Let's go through an example with
some different hyperparameters.

60
00:04:00.309 --> 00:04:05.259
Here you have a 5x5 input,
and we're going to apply

61
00:04:05.259 --> 00:04:09.978
max pooling with a filter size that's 3x3,
so

62
00:04:09.978 --> 00:04:13.675
f = 3, and let's use a stride of 1.

63
00:04:13.675 --> 00:04:16.825
So in this case the output
size is going to be 3x3.

64
00:04:19.200 --> 00:04:21.740
And the formula is where it
develops in the previous videos for

65
00:04:21.740 --> 00:04:24.150
figuring out the output size for
a cons layer.

66
00:04:24.150 --> 00:04:26.129
Those formulas also work for max pooling.

67
00:04:26.129 --> 00:04:34.080
All right, so
that's n + 2p- f over s for Or + 1.

68
00:04:34.080 --> 00:04:38.520
That formula also works to figure
out the output size of max pool.

69
00:04:38.520 --> 00:04:43.630
But in this example let's compute each
of the elements of this 3x3 output.

70
00:04:43.630 --> 00:04:46.640
The upper left hand elements,
we're going to look over that region.

71
00:04:46.640 --> 00:04:50.319
So notice this is a 3x3 region
because of filter size is 3,

72
00:04:50.319 --> 00:04:53.216
and take the max there,
so that's going to be 9.

73
00:04:53.216 --> 00:04:58.920
And then we shift it over by 1
because we can take stride of 1,

74
00:04:58.920 --> 00:05:02.170
so that max in the blue box is 9.

75
00:05:02.170 --> 00:05:03.860
Let's shift that over again.

76
00:05:03.860 --> 00:05:06.570
The max of the blue box is 5.

77
00:05:06.570 --> 00:05:09.328
And then let's go on to the next row.

78
00:05:09.328 --> 00:05:13.624
A stride of 1, so
we're just stepping down by one step.

79
00:05:13.624 --> 00:05:17.649
So max in that region is 9,
max in that region is 9,

80
00:05:17.649 --> 00:05:22.422
max in that region,
there's two 5s, the max is the 5.

81
00:05:22.422 --> 00:05:27.590
And then finally, max in that is 8,
max in that is 6.

82
00:05:27.590 --> 00:05:31.950
And max in that does not give you
does not give you the red corner.

83
00:05:31.950 --> 00:05:36.566
Okay, so this,
with this as the high parameters f = 3,

84
00:05:36.566 --> 00:05:39.560
s = 1, gives that output as shown.

85
00:05:39.560 --> 00:05:45.230
Now so far,
I've shown max pooling on a 2D input.

86
00:05:45.230 --> 00:05:51.830
If you have a 3D input, then the output
will have the same dimension.

87
00:05:53.450 --> 00:05:59.094
So for example, if you have 5x5x2,
then the output will be 3x3x2.

88
00:06:00.719 --> 00:06:05.529
And the way you compute max pooling is you
perform the computation we just described

89
00:06:05.529 --> 00:06:08.570
on each of the channels independently.

90
00:06:08.570 --> 00:06:11.910
So we have the first channel, which is
shown here on top, is still the same.

91
00:06:11.910 --> 00:06:16.080
And then for the second channel, I guess
this one that I just drew at the bottom,

92
00:06:16.080 --> 00:06:20.310
you would do the same computation
on that slice of this volume.

93
00:06:20.310 --> 00:06:23.450
And that gives you this second slice.

94
00:06:24.840 --> 00:06:29.863
And more generally, if this was
5x5 by some number of channels,

95
00:06:29.863 --> 00:06:35.160
the output will be 3x3 by
that same number of channels.

96
00:06:35.160 --> 00:06:39.864
And the max pooling computation
is done independently on

97
00:06:39.864 --> 00:06:42.120
each of these nc channels.

98
00:06:44.560 --> 00:06:46.776
So that's max pooling.

99
00:06:46.776 --> 00:06:50.510
There's one other type of pooling that
isn't used very often that I'll mention

100
00:06:50.510 --> 00:06:52.920
briefly, which is average pooling.

101
00:06:52.920 --> 00:06:55.890
So that's pretty much what you'd expect,
which is

102
00:06:57.030 --> 00:07:02.120
instead of taking the maxes within
each filter, you take the average.

103
00:07:02.120 --> 00:07:05.225
So in this example, the average
of the numbers in purple is 3.75,

104
00:07:05.225 --> 00:07:07.130
then there's 1.25, and 4, and 2.

105
00:07:07.130 --> 00:07:14.082
And so this is average pooling
with hyperparameters f = 2,

106
00:07:14.082 --> 00:07:20.370
s = 2, we can choose other
hyperparameters as well.

107
00:07:22.170 --> 00:07:27.140
So these days max pooling is used
much more often than average pooling,

108
00:07:27.140 --> 00:07:32.230
with one exception, which is sometimes,
very deep in a neural network,

109
00:07:32.230 --> 00:07:36.735
you might use average pooling to
collapse your implementation from, say,

110
00:07:36.735 --> 00:07:40.590
7x7x1000.

111
00:07:40.590 --> 00:07:44.770
And average over all the spacial
extents to get 1x1x1000.

112
00:07:44.770 --> 00:07:47.030
We'll see an example of this later.

113
00:07:48.140 --> 00:07:53.200
But you see max pooling used much more in
the neural network than average pooling.

114
00:07:54.520 --> 00:07:58.690
So just to summarize,
the hyperparameters for

115
00:07:58.690 --> 00:08:03.160
pooling are f, the filter size,
and s, the stride.

116
00:08:03.160 --> 00:08:06.650
And maybe common choices of
parameters might be f = 2, s = 2.

117
00:08:06.650 --> 00:08:09.640
This is used quite often.

118
00:08:09.640 --> 00:08:13.250
And this is has the effect of
roughly shrinking the height and

119
00:08:13.250 --> 00:08:16.400
width By a factor of about two.

120
00:08:16.400 --> 00:08:20.550
And the common choice of
hyperparameters might be f = 2, s = 2,.

121
00:08:20.550 --> 00:08:24.800
And this has the effect of
shrinking the height and

122
00:08:24.800 --> 00:08:28.620
width of the representation
by a factor of two.

123
00:08:28.620 --> 00:08:32.285
Have also seen f = 3, s = 2 used.

124
00:08:32.285 --> 00:08:36.800
And then the other hyperparameter
is just the combined bit that

125
00:08:36.800 --> 00:08:41.260
says are you using max pooling or
are you using average pooling?

126
00:08:41.260 --> 00:08:44.621
If you want you can add
an extra hyperparameter for

127
00:08:44.621 --> 00:08:48.470
the padding, although this is very,
very rarely used.

128
00:08:48.470 --> 00:08:51.770
When you do max pooling,
usually you do not use any padding.

129
00:08:51.770 --> 00:08:55.170
Although, there is one exception
that we'll see next week as well.

130
00:08:55.170 --> 00:08:59.630
But for the most part, max pooling
usually does not use any padding.

131
00:08:59.630 --> 00:09:04.679
So the most common value of p,
by far, is p equals 0.

132
00:09:06.466 --> 00:09:11.350
And the input of max pooling is
that you input the volume of size

133
00:09:12.660 --> 00:09:15.710
that and each by nw and nc, and

134
00:09:15.710 --> 00:09:21.400
it would output a volume of
size given by this, right.

135
00:09:21.400 --> 00:09:26.618
So assuming there's no padding,
x nw- f over s + 1 for x nc.

136
00:09:26.618 --> 00:09:32.038
And so the number of input
channels is equal to the number of

137
00:09:32.038 --> 00:09:39.790
output channels because pooling applies
to each of your channels independently.

138
00:09:41.390 --> 00:09:47.140
One thing to note about pooling is that
there are no parameters to learn, right.

139
00:09:47.140 --> 00:09:49.585
So when you implement backprop,

140
00:09:49.585 --> 00:09:55.360
you find that there are no parameters
that backprop will adapt to max pooling.

141
00:09:55.360 --> 00:09:58.509
Instead there are just these hyper
parameters that you set once,

142
00:09:58.509 --> 00:10:01.630
maybe set once by hand or
set using cross validation.

143
00:10:01.630 --> 00:10:03.700
And then, beyond that, you're done.

144
00:10:03.700 --> 00:10:08.300
It's just a fixed function that the neural
network computes in one of the layers.

145
00:10:08.300 --> 00:10:11.050
And there is actually nothing to learn,
it's just a fixed function.

146
00:10:12.500 --> 00:10:14.435
So that's it for pooling.

147
00:10:14.435 --> 00:10:16.287
So that's it for pooling,

148
00:10:16.287 --> 00:10:21.135
we now know how to build convulational
layers and pooling layers.

149
00:10:21.135 --> 00:10:24.215
In the next video, let's see a more
complex example of a ConvNet,

150
00:10:24.215 --> 00:10:28.175
one that will also allow us to
introduce fully connected layers